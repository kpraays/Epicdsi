**Foundation model:** 

“A* foundation model is any model trained on a large spectrum of data that can be adapted to a wide range of tasks*” **{[https://arxiv.org/pdf/2108.07258](https://arxiv.org/pdf/2108.07258) - rishibommasani_2021_on}**. The key aspects being the scale of data used and the huge scope of applications. These models implicitly gain the understanding of trends within data and by the sheer scale, they can generalize across different tasks for which they were not originally trained. They can use the implicitly understood trends to better model the outcome. This capability to generalize is their most important advantage.  

They are developed using large amounts of data and transfer learning (which aims to extract knowledge from one task and apply it to the target task **{[ https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf) - pan_2010_a}**). Pretraining is the approach used for making these models by training them on a surrogate task and then adapting them for the intended task through fine-tuning. When the pretraining task is done using unannotated data, it falls under self-supervised learning. Because the key aspect in making foundation models is the sheer volume of data needed, self-supervised learning approach to training makes them feasible. These models are intended to serve as the starting point for many different application or downstream tasks **{[https://arxiv.org/pdf/2108.07258](https://arxiv.org/pdf/2108.07258) - rishibommasani_2021_on}.** 

In the context of Human activity recognition (HAR), unannotated human activity data is available for much cheaper than labeled data thereby making self-supervised learning approach very attractive. Encapsulating the understanding of accelerometer data for a population group at scale in a foundation model trained using self-supervised learning such that it can be used for a variety of HAR related tasks without requiring extensive retraining from scratch for each group and task is very useful. 

 

**Why is multi-task learning used for human activity recognition from accelerometer data?** 

Multi-task learning tries to enhance the learning efficiency and accuracy by optimizing multiple objectives based on shared representations among a set of tasks **{[https://arxiv.org/pdf/1907.11879](https://arxiv.org/pdf/1907.11879) - saeed_2019_multitask}**.  A multi-task learning framework will try to simultaneously learn multiple tasks even though different, by uncovering common features that benefit each task in contrast to transfer learning which aims to learn more for the target task such that the roles of the source and target tasks are no longer symmetric **{[ https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf) - pan_2010_a}.** 

In the context of human activity recognition using multi-task learning, this means that the problem needs to be modeled in a way such that the model can learn the characteristics of data allowing it to classify human activities. Usually, domain knowledge is used to extract the features from the raw data to make predictions. But framing the problem from a multi-task learning objective involves applying additional transformations to raw accelerometer data and feeding it to a convolutional network architecture whose task is to determine which transformations have been applied to its input. While doing so, the network will learn the latent hidden representation of the input data. It allows the network to capture core signal characteristics by teaching it to detect high-level semantics, sensor behavior under different device placements, time-shifting of the events, varying amplitudes, and robustness against sensor noise **{https://arxiv.org/pdf/2202.12938 - haresamudram_2022_assessing}**. This representation is needed for classifying human activities, referred to as the downstream task while the transformation identification is a surrogate task in achieving our objective (also called pretext). It follows the pretrain-then-finetune paradigm and thus can use large amounts of unlabeled data for learning representations **{https://arxiv.org/pdf/2202.12938 - haresamudram_2022_assessing}.** 

 

**How will finetuning be performed?** 

Once a self-supervised learnt foundation model is trained using multi-task learning problem objectives, the captured hidden representations can be used for downstream tasks. These tasks involve recognition of human activities. In this work,** {[https://arxiv.org/pdf/1907.11879](https://arxiv.org/pdf/1907.11879) - saeed_2019_multitask}**, a classifier layer was added on top of the hidden layers of the pre-trained model for obtaining the labeled outputs. Furthermore, they also showed that finetuning the pre-trained model may lead to performance improvement which formed the basis of this work **{[https://www.nature.com/articles/s41746-024-01062-3](https://www.nature.com/articles/s41746-024-01062-3) - yuan_2024_selfsupervised}**.  

Here, finetuning means changing the weights of the pre-trained model either partially or completely based on labelled downstream datasets to increase the performance of model on the final intended tasks. This allows the layers of pre-trained model (depending on which layers are kept unfrozen) to adapt themselves to perform better for the fine-tuned tasks. Finetuning is distinct from adding a trained classifier layer to the pre-trained model because it involves modification of the weights of the model while trained classifier does not touch the fine-tuned model. **{[https://www.nature.com/articles/s41746-024-01062-3](https://www.nature.com/articles/s41746-024-01062-3) - yuan_2024_selfsupervised}** finetuned all layers and fully connected layers after the ConV layers.  

In the context of our problem, there are other fine-tuning approaches such as fine-tuning a different subset of layers using gradual unfreezing, Parameter-Efficient Fine-Tuning, Soft Layer-Wise Regularization, etc. which can be utilized to explore performance differences. Lastly, the foundation model pre-trained on UKBioBank dataset can be fine-tuned with much lesser amounts of Canada specific labelled data and its performance gains can be explored without expending as much resources as creating a huge dataset UKBioBank from scratch for Canadian contexts. This can be executed by building on top of software artefacts released along with this work **{[https://www.nature.com/articles/s41746-024-01062-3](https://www.nature.com/articles/s41746-024-01062-3) - yuan_2024_selfsupervised}**.

 

**What is feature interpretation?**

Since the network makes the predictions based on the latent features it has learnt from the pretext tasks, we will want to understand which features played a greater role in classifying the output activity. This is where feature interpretation comes into play. Determining which sets of features were influential in predicting which activities. For time series-based data, such a visualization is difficult due to lack of spatial symmetry between the input and the outputs. This is why a video-based approach correlating the activity being performed with the epochs classified was applied along with explainable AI techniques to determine which features in the input were most influential ([https://www.nature.com/articles/s41746-024-01062-3](https://www.nature.com/articles/s41746-024-01062-3)).  
