{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64c37ad-06e8-4134-bb77-544eb138bc05",
   "metadata": {},
   "source": [
    "- Generate epoch files with epoch length as 10 seconds.\n",
    "- Load the epoch files and label files with 10 second worth of labels.\n",
    "- Train the model.\n",
    "- Use the model to make predictions on test set and compare with actual annotations we have.\n",
    "- Make the confusion matrix.\n",
    "- Do it for: original model and custom features model.\n",
    "- Use those models for 22 random participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b341aaf-39f0-4571-a666-0273aafe023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(data_path) -> None:\n",
    "    total_csv_zipped = []\n",
    "    for path, dirnames, filenames in os.walk(data_path):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".csv.gz\"):\n",
    "                total_csv_zipped.append(os.path.join(path, file))\n",
    "                \n",
    "            if file.endswith(\".csv\") and file[0]!='c': #ignore the capture24 file\n",
    "                # paths contains the base directory for that file.\n",
    "                # dirnames contains other directories within this folder.\n",
    "                # filenames contains the list of filenames within path.\n",
    "                total_csv_zipped.append(os.path.join(path, file))\n",
    "    return sorted(total_csv_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e643ae-e494-40f2-9f63-339ba117b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "epoch_size = 1000 # 10 seconds\n",
    "\n",
    "epochs = \"/home/aayush/accelerometer/compare_classification/epoch_data/10_sec/with_extra_features/epoch_data\"\n",
    "epoch_files = get_files(epochs)\n",
    "\n",
    "labels = \"/home/yacine/accel/capture24/participants/\"\n",
    "label_files = get_files(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8635f7ad-6ebe-40c8-bb9e-1100befdb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_labels_dict(labels_dict_location = \"/home/aayush/accelerometer/accprocess/anno-label.csv\"):\n",
    "    labels_dict = {}\n",
    "    with open(labels_dict_location, \"r\") as annotation_dict:\n",
    "        reader = csv.DictReader(annotation_dict)\n",
    "        for row in reader:\n",
    "            if labels_dict.get(row['annotation']) is None:\n",
    "                labels_dict[row['annotation']] = [row['label:Walmsley2020']]\n",
    "            else:\n",
    "                labels_dict[row['annotation']].append(row['label:Walmsley2020'])\n",
    "    return labels_dict\n",
    "\n",
    "labels_dict = create_labels_dict()\n",
    "\n",
    "# replace the annotated labels with the same format of strings as predicted labels using mapping from labels_dict\n",
    "flat_dict = {k: v[0] for k, v in labels_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3c7b2e-23c8-4749-a903-dde54ae43ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datetime(dt_string):\n",
    "    clean_datetime_str = dt_string.split('[')[0].strip()\n",
    "    dt_object = pd.to_datetime(clean_datetime_str)\n",
    "    return dt_object\n",
    "\n",
    "\n",
    "def parse_datetime_df_time(dt_string):\n",
    "    clean_datetime_str = dt_string.split('[')[0].strip()\n",
    "    clean_datetime_str = clean_datetime_str.split('+')[0].strip()\n",
    "    dt_object = pd.to_datetime(clean_datetime_str)\n",
    "    return dt_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbda6a50-6f63-4108-956f-9e63cd1dca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3442: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/151 done...\n",
      "2/151 done...\n",
      "3/151 done...\n",
      "4/151 done...\n",
      "5/151 done...\n",
      "6/151 done...\n",
      "7/151 done...\n",
      "8/151 done...\n",
      "9/151 done...\n",
      "10/151 done...\n",
      "11/151 done...\n",
      "12/151 done...\n",
      "13/151 done...\n",
      "14/151 done...\n",
      "15/151 done...\n",
      "16/151 done...\n",
      "17/151 done...\n",
      "18/151 done...\n",
      "19/151 done...\n",
      "20/151 done...\n",
      "21/151 done...\n",
      "22/151 done...\n",
      "23/151 done...\n",
      "24/151 done...\n",
      "25/151 done...\n",
      "26/151 done...\n",
      "27/151 done...\n",
      "28/151 done...\n",
      "29/151 done...\n",
      "30/151 done...\n",
      "31/151 done...\n",
      "32/151 done...\n",
      "33/151 done...\n",
      "34/151 done...\n",
      "35/151 done...\n",
      "36/151 done...\n",
      "37/151 done...\n",
      "38/151 done...\n",
      "39/151 done...\n",
      "40/151 done...\n",
      "41/151 done...\n",
      "42/151 done...\n",
      "43/151 done...\n",
      "44/151 done...\n",
      "45/151 done...\n",
      "46/151 done...\n",
      "47/151 done...\n",
      "48/151 done...\n",
      "49/151 done...\n",
      "50/151 done...\n",
      "51/151 done...\n",
      "52/151 done...\n",
      "53/151 done...\n",
      "54/151 done...\n",
      "55/151 done...\n",
      "56/151 done...\n",
      "57/151 done...\n",
      "58/151 done...\n",
      "59/151 done...\n",
      "60/151 done...\n",
      "61/151 done...\n",
      "62/151 done...\n",
      "63/151 done...\n",
      "64/151 done...\n",
      "65/151 done...\n",
      "66/151 done...\n",
      "67/151 done...\n",
      "68/151 done...\n",
      "69/151 done...\n",
      "70/151 done...\n",
      "71/151 done...\n",
      "72/151 done...\n",
      "73/151 done...\n",
      "74/151 done...\n",
      "75/151 done...\n",
      "76/151 done...\n",
      "77/151 done...\n",
      "78/151 done...\n",
      "79/151 done...\n",
      "80/151 done...\n",
      "81/151 done...\n",
      "82/151 done...\n",
      "83/151 done...\n",
      "84/151 done...\n",
      "85/151 done...\n",
      "86/151 done...\n",
      "87/151 done...\n",
      "88/151 done...\n",
      "89/151 done...\n",
      "90/151 done...\n",
      "91/151 done...\n",
      "92/151 done...\n",
      "93/151 done...\n",
      "94/151 done...\n",
      "95/151 done...\n",
      "96/151 done...\n",
      "97/151 done...\n",
      "98/151 done...\n",
      "99/151 done...\n",
      "100/151 done...\n",
      "101/151 done...\n",
      "102/151 done...\n",
      "103/151 done...\n",
      "104/151 done...\n",
      "105/151 done...\n",
      "106/151 done...\n",
      "107/151 done...\n",
      "108/151 done...\n",
      "109/151 done...\n",
      "110/151 done...\n",
      "111/151 done...\n",
      "112/151 done...\n",
      "113/151 done...\n",
      "114/151 done...\n",
      "115/151 done...\n",
      "116/151 done...\n",
      "117/151 done...\n",
      "118/151 done...\n",
      "119/151 done...\n",
      "120/151 done...\n",
      "121/151 done...\n",
      "122/151 done...\n",
      "123/151 done...\n",
      "124/151 done...\n",
      "125/151 done...\n",
      "126/151 done...\n",
      "127/151 done...\n",
      "128/151 done...\n",
      "129/151 done...\n",
      "130/151 done...\n",
      "131/151 done...\n",
      "132/151 done...\n",
      "133/151 done...\n",
      "134/151 done...\n",
      "135/151 done...\n",
      "136/151 done...\n",
      "137/151 done...\n",
      "138/151 done...\n",
      "139/151 done...\n",
      "140/151 done...\n",
      "141/151 done...\n",
      "142/151 done...\n",
      "143/151 done...\n",
      "144/151 done...\n",
      "145/151 done...\n",
      "146/151 done...\n",
      "147/151 done...\n",
      "148/151 done...\n",
      "149/151 done...\n",
      "150/151 done...\n",
      "151/151 done...\n"
     ]
    }
   ],
   "source": [
    "all_epoch_dfs = []\n",
    "for i, (epochfilename, labelfilename) in enumerate(zip(epoch_files, label_files)):\n",
    "    epoch_df = pd.read_csv(epochfilename)\n",
    "    label_df = pd.read_csv(labelfilename)\n",
    "\n",
    "    # Take the timestamp after every thirty seconds\n",
    "    label_df = label_df[[\"annotation\", \"time\"]][0::epoch_size]\n",
    "    \n",
    "    # Convert to datetime object\n",
    "    label_df[\"time\"] = label_df[\"time\"].apply(parse_datetime)\n",
    "\n",
    "    # replace the label with the categories we expect\n",
    "    label_df['annotation'] = label_df['annotation'].replace(flat_dict)\n",
    "\n",
    "    # convert epoch df time stamp with datetime object\n",
    "    epoch_df['time'] = epoch_df['time'].apply(parse_datetime_df_time)\n",
    "    epoch_df[\"participant\"] = float(epochfilename.split(\"/\")[-1].split(\"-\")[0][1:])\n",
    "\n",
    "    # predropped_epoch = len(epoch_df)\n",
    "    # predropped_label = len(label_df)\n",
    "    \n",
    "    # # keep only those time stamps which are present in both epoch and label df\n",
    "    # label_df = label_df[label_df[\"time\"].isin(epoch_df['time'])]\n",
    "    # epoch_df = epoch_df[epoch_df[\"time\"].isin(label_df['time'])]\n",
    "\n",
    "    # print(f\"We removed {predropped_label-len(label_df)} rows from label df because timestamps were not in epoch df.\")\n",
    "    # print(f\"We removed {predropped_epoch-len(epoch_df)} rows from epoch_df because timestamps were not in label_df.\")\n",
    "    \n",
    "    label_df = label_df[[\"time\", \"annotation\"]]\n",
    "    label_df.set_index(\"time\", inplace=True)\n",
    "    epoch_df.set_index(\"time\", inplace=True)\n",
    "    \n",
    "    # epoch_df[\"label\"] = label_df['annotation']\n",
    "    epoch_df=epoch_df.join(label_df, on=\"time\")\n",
    "    \n",
    "    all_epoch_dfs.append(epoch_df)\n",
    "    print(f\"{i+1}/{len(epoch_files)} done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0bd7543-393b-47c3-8c50-8d299b359931",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = pd.concat(all_epoch_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ffae7a-a5e5-436e-b1a6-f46b7fe7da61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enmoTrunc</th>\n",
       "      <th>enmoAbs</th>\n",
       "      <th>xMean</th>\n",
       "      <th>yMean</th>\n",
       "      <th>zMean</th>\n",
       "      <th>xRange</th>\n",
       "      <th>yRange</th>\n",
       "      <th>zRange</th>\n",
       "      <th>xStd</th>\n",
       "      <th>yStd</th>\n",
       "      <th>...</th>\n",
       "      <th>p625</th>\n",
       "      <th>totalPower</th>\n",
       "      <th>temp</th>\n",
       "      <th>samples</th>\n",
       "      <th>dataErrors</th>\n",
       "      <th>clipsBeforeCalibr</th>\n",
       "      <th>clipsAfterCalibr</th>\n",
       "      <th>rawSamples</th>\n",
       "      <th>participant</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-11-13 02:18:00</th>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>-0.460820</td>\n",
       "      <td>-0.523185</td>\n",
       "      <td>0.714398</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.031827</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.894722</td>\n",
       "      <td>-14.462759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-13 02:18:10</th>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>-0.461200</td>\n",
       "      <td>-0.523639</td>\n",
       "      <td>0.714542</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.031827</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.949640</td>\n",
       "      <td>-14.751847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-13 02:18:20</th>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>-0.463512</td>\n",
       "      <td>-0.520434</td>\n",
       "      <td>0.715178</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.031827</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.121028</td>\n",
       "      <td>-15.372644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-13 02:18:30</th>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>-0.463957</td>\n",
       "      <td>-0.519730</td>\n",
       "      <td>0.715366</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.031827</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.189799</td>\n",
       "      <td>-15.489662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-13 02:18:40</th>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>-0.465270</td>\n",
       "      <td>-0.519528</td>\n",
       "      <td>0.715324</td>\n",
       "      <td>0.015968</td>\n",
       "      <td>0.031282</td>\n",
       "      <td>0.032034</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.755043</td>\n",
       "      <td>-14.841447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sleep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     enmoTrunc   enmoAbs     xMean     yMean     zMean  \\\n",
       "time                                                                     \n",
       "2016-11-13 02:18:00   0.000843  0.003434 -0.460820 -0.523185  0.714398   \n",
       "2016-11-13 02:18:10   0.000818  0.002860 -0.461200 -0.523639  0.714542   \n",
       "2016-11-13 02:18:20   0.000639  0.002645 -0.463512 -0.520434  0.715178   \n",
       "2016-11-13 02:18:30   0.000597  0.002615 -0.463957 -0.519730  0.715366   \n",
       "2016-11-13 02:18:40   0.000793  0.002516 -0.465270 -0.519528  0.715324   \n",
       "\n",
       "                       xRange    yRange    zRange      xStd      yStd  ...  \\\n",
       "time                                                                   ...   \n",
       "2016-11-13 02:18:00  0.015839  0.031264  0.031827  0.005733  0.006868  ...   \n",
       "2016-11-13 02:18:10  0.015839  0.031264  0.031827  0.006072  0.007069  ...   \n",
       "2016-11-13 02:18:20  0.015839  0.031264  0.031827  0.007419  0.004226  ...   \n",
       "2016-11-13 02:18:30  0.015904  0.031264  0.031827  0.007570  0.003598  ...   \n",
       "2016-11-13 02:18:40  0.015968  0.031282  0.032034  0.007853  0.004035  ...   \n",
       "\n",
       "                          p625  totalPower  temp  samples  dataErrors  \\\n",
       "time                                                                    \n",
       "2016-11-13 02:18:00 -17.894722  -14.462759   0.0     1000           0   \n",
       "2016-11-13 02:18:10 -17.949640  -14.751847   0.0     1000           0   \n",
       "2016-11-13 02:18:20 -18.121028  -15.372644   0.0     1000           0   \n",
       "2016-11-13 02:18:30 -18.189799  -15.489662   0.0     1000           0   \n",
       "2016-11-13 02:18:40 -17.755043  -14.841447   0.0     1000           0   \n",
       "\n",
       "                     clipsBeforeCalibr  clipsAfterCalibr  rawSamples  \\\n",
       "time                                                                   \n",
       "2016-11-13 02:18:00                  0                 0        1001   \n",
       "2016-11-13 02:18:10                  0                 0        1002   \n",
       "2016-11-13 02:18:20                  0                 0        1002   \n",
       "2016-11-13 02:18:30                  0                 0        1002   \n",
       "2016-11-13 02:18:40                  0                 0        1002   \n",
       "\n",
       "                     participant  annotation  \n",
       "time                                          \n",
       "2016-11-13 02:18:00          1.0       sleep  \n",
       "2016-11-13 02:18:10          1.0       sleep  \n",
       "2016-11-13 02:18:20          1.0       sleep  \n",
       "2016-11-13 02:18:30          1.0       sleep  \n",
       "2016-11-13 02:18:40          1.0       sleep  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = epochs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c630412-ad4b-4a88-abe0-eee677ed6ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1398036\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={'annotation':'label'}, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d37f111a-8cc4-44cf-b2e6-e682fd9abd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sleep' nan 'light' 'sedentary' 'moderate-vigorous']\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1050a1cd-82bd-4e49-bf1b-e2c81fa9b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(labels=[\"time\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "115dbbc2-92a6-4209-8c4c-b8f7c7791daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sleep' 'light' 'sedentary' 'moderate-vigorous']\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=[\"label\"])\n",
    "print(df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c42d12-7d04-43dc-9521-44d71035d866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883480"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"label\"] = df[\"label\"].astype('string', copy=False)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8e6b3b-9f8d-4545-90bd-7f45e710eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/aayush/accelerometer/compare_classification/training_data/10_sec/extra_features/10s_training_with_annotation_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c87096-ceda-4f98-b04b-192a8b124037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291961\n",
      "['sleep' 'sedentary' 'light' 'moderate-vigorous']\n"
     ]
    }
   ],
   "source": [
    "test_participants = \"101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151\"\n",
    "test_participants = [float(t) for t in test_participants.split(\",\")]\n",
    "    \n",
    "test_features_df = df[df[\"participant\"].isin(test_participants)].iloc[:, :-2]\n",
    "test_labels_df = df[df[\"participant\"].isin(test_participants)][\"label\"]\n",
    "\n",
    "print(len(test_features_df))\n",
    "print(test_labels_df.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18a8c635-3eef-4bd2-9b78-326fbc5cd19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591519\n",
      "time\n",
      "2016-11-13 02:18:00    sleep\n",
      "2016-11-13 02:18:10    sleep\n",
      "2016-11-13 02:18:20    sleep\n",
      "2016-11-13 02:18:30    sleep\n",
      "2016-11-13 02:18:40    sleep\n",
      "                       ...  \n",
      "2016-05-31 02:13:20    sleep\n",
      "2016-05-31 02:13:30    sleep\n",
      "2016-05-31 02:13:40    sleep\n",
      "2016-05-31 02:13:50    sleep\n",
      "2016-05-31 02:14:00    sleep\n",
      "Name: label, Length: 591519, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_all_df = df[~df[\"participant\"].isin(test_participants)]\n",
    "train_features_df = df[~df[\"participant\"].isin(test_participants)].iloc[:, :-2]\n",
    "train_labels_predict = df[~df[\"participant\"].isin(test_participants)][\"label\"]\n",
    "\n",
    "print(len(train_features_df))\n",
    "print(train_labels_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a087d5-a46a-4a7a-b371-8399e68020b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Model(**kwargs):\n",
    "    return BalancedRandomForestClassifier(\n",
    "        n_estimators=3000,\n",
    "        max_depth=10,\n",
    "        min_samples_leaf=1,\n",
    "        replacement=True,\n",
    "        sampling_strategy='not minority',\n",
    "        random_state=42,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88d57b44-78bb-4065-8938-461236c046d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=10)]: Done 2430 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=10)]: Done 3000 out of 3000 | elapsed:  8.8min finished\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "print('Training...')\n",
    "model = _Model(n_jobs=10, verbose=1)\n",
    "# fit the model as numpy array so that we do not get warnings during prediction\n",
    "X_features = train_features_df.to_numpy()\n",
    "X_labels = train_labels_predict.to_numpy()\n",
    "model = model.fit(X_features, X_labels)\n",
    "model.verbose = 0  # silence future calls to .predict()\n",
    "labels = model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2871c93-f583-41dc-99e1-d8c9237f62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light' 'moderate-vigorous' 'sedentary' 'sleep']\n",
      "591519\n",
      "591519\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "pid = df[~df[\"participant\"].isin(test_participants)][\"participant\"].to_numpy()\n",
    "print(len(pid))\n",
    "print(len(train_labels_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4bbd643-5918-43e2-ad80-55f5c1abd0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToTar(tarOut, **kwargs):\n",
    "    \"\"\"Save objects to tar file. Objects must be passed as keyworded arguments,\n",
    "    then the key is used for the object name in the tar file.\n",
    "\n",
    "    :param **kwargs: Objects to be saved passed as keyworded arguments.\n",
    "\n",
    "    :return: tar file written to <tarOut>\n",
    "    :rtype: void\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "        with tarfile.open(tarOut, mode='w') as tf:\n",
    "\n",
    "            for key, val in kwargs.items():\n",
    "                pth = os.path.join(tmpdir, key)\n",
    "                joblib.dump(val, pth, compress=True)\n",
    "                tf.add(pth, arcname=key)\n",
    "\n",
    "        print('Models saved to', tarOut)\n",
    "\n",
    "    finally:\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree(tmpdir)\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "\n",
    "def getFileFromTar(tarArchive, targetFile):\n",
    "    \"\"\"Read file from tar\n",
    "\n",
    "    This is currently more tricky than it should be see\n",
    "    https://github.com/numpy/numpy/issues/7989\n",
    "\n",
    "    :param str tarArchive: Input tarfile object\n",
    "    :param str targetFile: Target individual file within .tar\n",
    "\n",
    "    :return: file object byte stream\n",
    "    :rtype: object\n",
    "    \"\"\"\n",
    "\n",
    "    with tarfile.open(tarArchive, 'r') as t:\n",
    "        b = BytesIO()\n",
    "        try:\n",
    "            b.write(t.extractfile(targetFile).read())\n",
    "        except KeyError:\n",
    "            return None\n",
    "        b.seek(0)\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "def trainHMM(Y_prob, Y_true, labels=None, uniform_prior=True):\n",
    "    \"\"\" https://en.wikipedia.org/wiki/Hidden_Markov_model\n",
    "\n",
    "    :return: Dictionary containing prior, emission and transition\n",
    "        matrices, and corresponding labels.\n",
    "    :rtype: dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(Y_true)\n",
    "\n",
    "    if uniform_prior:\n",
    "        # All labels with equal probability\n",
    "        prior = np.ones(len(labels)) / len(labels)\n",
    "    else:\n",
    "        # Label probability equals empirical rate\n",
    "        prior = np.mean(Y_true.reshape(-1, 1) == labels, axis=0)\n",
    "\n",
    "    emission = np.vstack(\n",
    "        [np.mean(Y_prob[Y_true == label], axis=0) for label in labels]\n",
    "    )\n",
    "    transition = np.vstack(\n",
    "        [np.mean(Y_true[1:][(Y_true == label)[:-1]].reshape(-1, 1) == labels, axis=0)\n",
    "            for label in labels]\n",
    "    )\n",
    "\n",
    "    params = {'prior': prior, 'emission': emission, 'transition': transition, 'labels': labels}\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b97bf4c-9a3b-474f-a55e-43e9b8cf6f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-predicting to derive the observations for HMM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   5 out of  10 | elapsed: 24.2min remaining: 24.2min\n",
      "[Parallel(n_jobs=5)]: Done  10 out of  10 | elapsed: 42.0min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print('Cross-predicting to derive the observations for HMM...')\n",
    "\n",
    "NJOBS_PER_CV_MODEL = min(2, 10)\n",
    "cvp = cross_val_predict(\n",
    "    _Model(n_jobs=NJOBS_PER_CV_MODEL), X_features, X_labels, groups=pid,\n",
    "    cv=10,\n",
    "    n_jobs=10 // NJOBS_PER_CV_MODEL,\n",
    "    method=\"predict_proba\",\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a9580ae-7fc6-4e32-a823-c1148e0bce75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['light', 'moderate-vigorous', 'sedentary', 'sleep'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c601f0f-4620-48c8-a199-5cbcb8dee04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM...\n"
     ]
    }
   ],
   "source": [
    "print('Training HMM...')\n",
    "# train_labels_predict_array = np.array(train_labels_predict)\n",
    "# hmmParams = trainHMM(cvp,  train_labels_predict_array)\n",
    "\n",
    "hmmParams = trainHMM(cvp,  X_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "032fcb6d-744c-48f2-a742-66dc7c4c454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prior': array([0.25, 0.25, 0.25, 0.25]),\n",
       " 'emission': array([[0.33993685, 0.28638068, 0.291851  , 0.08183147],\n",
       "        [0.30721581, 0.36509814, 0.26417025, 0.0635158 ],\n",
       "        [0.2908913 , 0.25157763, 0.35140236, 0.10612872],\n",
       "        [0.08167573, 0.06034193, 0.10836656, 0.74961579]]),\n",
       " 'transition': array([[9.82898529e-01, 3.09350273e-03, 1.35765298e-02, 4.31438066e-04],\n",
       "        [1.25358707e-02, 9.85160852e-01, 2.15224286e-03, 1.51034587e-04],\n",
       "        [5.79335573e-03, 2.06763554e-04, 9.93805046e-01, 1.94834887e-04],\n",
       "        [3.61679562e-04, 1.95502466e-05, 1.07526356e-04, 9.99511244e-01]]),\n",
       " 'labels': array(['light', 'moderate-vigorous', 'sedentary', 'sleep'], dtype=object)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmmParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95aa86d6-1861-4ba8-affa-7b4c65aee5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'light': 2.270833333333333,\n",
       " 'moderate-vigorous': 4.682608695652173,\n",
       " 'sedentary': 1.5634920634920637,\n",
       " 'sleep': 0.9499999999999998}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# METs = {y: train_all_df[train_all_df[\"label\"] == y].groupby(\"label\")[\"MET\"].mean().mean() \n",
    "#     for y in model.classes_}\n",
    "METs = {'light': 2.270833333333333,\n",
    " 'moderate-vigorous': 4.682608695652173,\n",
    " 'sedentary': 1.5634920634920637,\n",
    " 'sleep': 0.9499999999999998}\n",
    "METs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c21fc588-9178-4c90-ac79-755710da3db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved to /home/aayush/accelerometer/compare_classification/accProcess_output/10_sec/extra_features/model_used/10s_extra_model.tar\n",
      "Output trained model written to: /home/aayush/accelerometer/compare_classification/accProcess_output/10_sec/extra_features/model_used/10s_extra_model.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import joblib\n",
    "import tarfile\n",
    "\n",
    "featureCols = np.array(train_features_df.columns)\n",
    "# Write model to file\n",
    "outFile = os.path.join(\"/home/aayush/accelerometer/compare_classification/accProcess_output/10_sec/extra_features/model_used\", '10s_extra_model.tar')\n",
    "saveToTar(outFile,\n",
    "          model=model,\n",
    "          labels=labels,\n",
    "          featureCols=featureCols,\n",
    "          hmmParams=hmmParams,\n",
    "          METs=METs)\n",
    "print(f'Output trained model written to: {outFile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8995be60-3bec-4094-822d-e666732d780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max          0.064296\n",
      "xRange       0.059305\n",
      "75thp        0.055442\n",
      "sd           0.047031\n",
      "minx         0.045065\n",
      "zRange       0.043988\n",
      "xStd         0.041275\n",
      "mean         0.036784\n",
      "fft2         0.031967\n",
      "zStd         0.028829\n",
      "fft1         0.027774\n",
      "xMean        0.024979\n",
      "yRange       0.023975\n",
      "fft3         0.022431\n",
      "maxx         0.022376\n",
      "miny         0.017885\n",
      "enmoAbs      0.016795\n",
      "MAD          0.016660\n",
      "minz         0.016036\n",
      "enmoTrunc    0.014866\n",
      "yawg         0.014275\n",
      "fft4         0.014265\n",
      "median       0.014052\n",
      "MPD          0.013791\n",
      "pmax         0.012818\n",
      "fft5         0.012681\n",
      "pitchg       0.012111\n",
      "maxz         0.011601\n",
      "yMean        0.011287\n",
      "fft9         0.010703\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display feature importances\n",
    "feature_importances = pd.Series(model.feature_importances_, index=df.columns[:-2])\n",
    "print(feature_importances.nlargest(30))  # Show the top 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09c382ec-c3e6-4413-849c-c471a950a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/.local/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but BalancedRandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            light       0.44      0.34      0.38     62361\n",
      "moderate-vigorous       0.17      0.42      0.24     18008\n",
      "        sedentary       0.67      0.55      0.60    113427\n",
      "            sleep       0.86      0.92      0.89     98165\n",
      "\n",
      "         accuracy                           0.62    291961\n",
      "        macro avg       0.53      0.56      0.53    291961\n",
      "     weighted avg       0.65      0.62      0.63    291961\n",
      "\n",
      "Score: 0.53\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# test the performance\n",
    "Y_labels_test_pred = model.predict(test_features_df)\n",
    "print(metrics.classification_report(test_labels_df, Y_labels_test_pred))\n",
    "testScore = metrics.f1_score(test_labels_df, Y_labels_test_pred, average='macro', zero_division=0)\n",
    "print(f'Score: {testScore:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1f50971-8d52-44f4-bcae-2ef53af914cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(Y_obs, hmm_params):\n",
    "    \"\"\" Perform HMM smoothing over observations via Viteri algorithm\n",
    "\n",
    "    https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "\n",
    "    :param dict hmm_params: Dictionary containing prior, emission and transition\n",
    "        matrices, and corresponding labels\n",
    "\n",
    "    :return: Smoothed sequence of activities\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "\n",
    "    def log(x):\n",
    "        SMALL_NUMBER = 1e-16\n",
    "        return np.log(x + SMALL_NUMBER)\n",
    "\n",
    "    prior = hmm_params['prior']\n",
    "    emission = hmm_params['emission']\n",
    "    transition = hmm_params['transition']\n",
    "    labels = hmm_params['labels']\n",
    "\n",
    "    nobs = len(Y_obs)\n",
    "    nlabels = len(labels)\n",
    "\n",
    "    Y_obs = np.where(Y_obs.reshape(-1, 1) == labels)[1]  # to numeric\n",
    "\n",
    "    probs = np.zeros((nobs, nlabels))\n",
    "    probs[0, :] = log(prior) + log(emission[:, Y_obs[0]])\n",
    "    for j in range(1, nobs):\n",
    "        for i in range(nlabels):\n",
    "            probs[j, i] = np.max(\n",
    "                log(emission[i, Y_obs[j]]) +\n",
    "                log(transition[:, i]) +\n",
    "                probs[j - 1, :])  # probs already in log scale\n",
    "    viterbi_path = np.zeros_like(Y_obs)\n",
    "    viterbi_path[-1] = np.argmax(probs[-1, :])\n",
    "    for j in reversed(range(nobs - 1)):\n",
    "        viterbi_path[j] = np.argmax(\n",
    "            log(transition[:, viterbi_path[j + 1]]) +\n",
    "            probs[j, :])  # probs already in log scale\n",
    "\n",
    "    viterbi_path = labels[viterbi_path]  # to labels\n",
    "\n",
    "    return viterbi_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0a4ab6f-4cac-4df9-bfdb-84afcc1b9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test performance (HMM):\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            light       0.51      0.33      0.40     62361\n",
      "moderate-vigorous       0.18      0.44      0.25     18008\n",
      "        sedentary       0.73      0.64      0.68    113427\n",
      "            sleep       0.89      0.99      0.93     98165\n",
      "\n",
      "         accuracy                           0.68    291961\n",
      "        macro avg       0.58      0.60      0.57    291961\n",
      "     weighted avg       0.70      0.68      0.68    291961\n",
      "\n",
      "Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "YpredHmm = viterbi(Y_labels_test_pred, hmmParams)\n",
    "\n",
    "print('\\nTest performance (HMM):')\n",
    "print(metrics.classification_report(test_labels_df, YpredHmm))\n",
    "testHmmScore = metrics.f1_score(test_labels_df, YpredHmm, average='macro', zero_division=0)\n",
    "print(f'Score: {testHmmScore:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
